{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hMXgC4iOc5RL"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Se1zeHeG-l0X"
      },
      "source": [
        "## Downloading the pre-trained model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2s1CC3TiderF"
      },
      "source": [
        "We import the model without the top layer, with the pretrained imagenet weights and an input shape of (299, 299, 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q36BZc2db-f",
        "outputId": "2bb60411-e066-4e1c-dd10-c59702271ded"
      },
      "outputs": [],
      "source": [
        "from keras.applications.xception import Xception\n",
        "base_model = Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(299, 299, 3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "llebdNxu_BAs"
      },
      "source": [
        "## Freeze the layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNbPgMcz_DXg"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsa_tsGd-tsq"
      },
      "source": [
        "## Add new layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This layers will be the ones that we will train at the beginning. This **new** part of the network will learn to identify the new classes that we want to classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EaSVt1Td-w8"
      },
      "outputs": [],
      "source": [
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D\n",
        "from keras.models import Model\n",
        "from keras import Input\n",
        "from keras.regularizers import l2\n",
        "\n",
        "inputs = Input(shape=(299, 299, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu',  kernel_regularizer=l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu',  kernel_regularizer=l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(6, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.inputs, outputs=predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RjOoCiwd_FX6"
      },
      "source": [
        "## Compile the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we 'save' the changes we have made to the model so we can tain it on its new form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlARiMob_G8H"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=categorical_crossentropy,\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WbmLARlgenN5"
      },
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes our data won't be enough to train a good model. In this case, we can use data augmentation to generate new images from the ones we already have. This will help the model to generalize better and avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mljbdr7bexZP"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.xception import preprocess_input\n",
        "## In case a custom preprocessing function is to be used\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rescale=1./225,\n",
        "#     rotation_range=25,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "train_datagen = ImageDataGenerator(dtype='float32',preprocessing_function=preprocess_input)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case we will use the preprocessing function that comes with the model. This function will normalize the images and will also do some data augmentation. In addition, we don't want to augment the validation data, so we will use two different preprocessing functions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uWhFBdZx5sIz"
      },
      "source": [
        "## Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcpxEJfn483l",
        "outputId": "4ad8c814-0ac3-4ba6-96fe-0c366c6bac73"
      },
      "outputs": [],
      "source": [
        "train_dir = 'your/path/to/train'\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=10,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_dir = 'your/path/to/validation'\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=10,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bmfW1D6D3O4a"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1EZ9QLiXAfgM"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Callbacks are functions that are called at certain points during the training process. We will use them to save the model when it achieves the best validation accuracy and to stop the training if the validation accuracy doesn't improve after a certain number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZgYxnZ1-tLd"
      },
      "outputs": [],
      "source": [
        "# Create a learning rate scheduler callback\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "\n",
        "exp_decay = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.5)\n",
        "lr_scheduler = LearningRateScheduler(exp_decay)\n",
        "\n",
        "# Create an early stopping callback\n",
        "patience = 2\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oum_YnhOAh2w"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training process can be computationally expensive, so we will use the GPU to speed up the process. The number of epochs is set to a really high number that we won't really achieve since EarlyStopping will stop the training when the validation loss doesn't improve after a certain number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swmozcnO5wBq",
        "outputId": "276997d6-3bc1-4b88-8015-a9e0295f4e53"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "    epochs=10000,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XPOs_4XwAkPv"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell is used as a security measure. If the fine tunning process is interrupted, we can save the model so we don't have to start the training from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyOA_0gKAnRA"
      },
      "outputs": [],
      "source": [
        "model.save('your/path/to/model.h5', save_format='h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Hh3qFIAm6u"
      },
      "source": [
        "# Fine tunning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we will be unfreezing some parts of the model and train them with a really low learning rate. This will help the model to learn more specific features and improve its accuracy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIj-069mlm9s"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('your/path/to/model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First iteration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e9SrHxttA06m"
      },
      "source": [
        "### Unfreezing parts of the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, we will unfreeze the 18 last layers of the model. This layers will be trained with a really low learning rate and learn more specific features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4EfJYXIADmj"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers[120:]:\n",
        "  layer.trainable = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lI5difT8A4Ed"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY5fU9r6JulW"
      },
      "outputs": [],
      "source": [
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "model.compile(\n",
        "    loss=categorical_crossentropy,\n",
        "    optimizer=Adam(learning_rate=0.000001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "zYfc_3WFe_oM",
        "outputId": "c17fcec6-93c0-4193-e9cd-1813505d53d9"
      },
      "outputs": [],
      "source": [
        "final_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n70h8VUIQ_-M"
      },
      "outputs": [],
      "source": [
        "model.save('your/path/to/model_phase1.h5', save_format='h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rest of the iterations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will be repeating the previous section on a while loop for code cleaness, but the process is the same: unfreezing layers 20 by 20 and training them with a really low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErAUQTV3RHo5",
        "outputId": "72099e32-0b66-4f97-c679-7b6e3d4eb6da"
      },
      "outputs": [],
      "source": [
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "n = 3\n",
        "\n",
        "while n > 0:\n",
        "  for layer in model.layers[n*20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "  model.compile(\n",
        "      loss=categorical_crossentropy,\n",
        "      optimizer=Adam(learning_rate=0.0001),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  final_history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "      epochs=25,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      callbacks=[early_stopping]\n",
        "  )\n",
        "  model.save(f\"your/path/to/model_phase{5-n}.h5\", save_format='h5')\n",
        "  print(f\"Model saved on your/path/to/model_phase{5-n}.h5\")\n",
        "  n = n-1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, we unfreeze the whole model and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz08MZutRe2C",
        "outputId": "61c4a0b2-6fcd-4975-a77b-0a9d8b4b29b5"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers[:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    loss=categorical_crossentropy,\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "final_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "model.save(f\"your/path/to/model_phase_final.h5\", save_format='h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have your own model you can test on your own test set via the `model.predict()` or `model.evaluate()` functions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hMXgC4iOc5RL"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
